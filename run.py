# -*- coding: utf-8 -*-
"""RunMe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F6dTwh1f-vJYJoEF9LBPUfblMBnICrH0

# Installation
"""

# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools
# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
# !apt-get update -qq 2>&1 > /dev/null
# !apt-get -y install -qq google-drive-ocamlfuse fuse
# from google.colab import auth
# auth.authenticate_user()
# from oauth2client.client import GoogleCredentials
# creds = GoogleCredentials.get_application_default()
# import getpass
# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
# vcode = getpass.getpass()
# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

import os, sys
from os.path import isfile, join
import numpy as np
from PIL import Image
import time
import sys
import warnings

if not sys.warnoptions:
    warnings.simplefilter("ignore")


PATH_INPUT = "input/"
PATH_OUTPUT = "output/"

original_imgs_test = PATH_INPUT

model_path = "model/weights.hdf5"
architechture_path = "model/architecture.json"

channels = 3
# height = 1860
# width = 1328

# height = 2336
# width = 1696


# dimension of the patches
patch_height = 32
patch_width = 32

#the stride in case output with average
stride_height = 20
stride_width = 20
assert (stride_height < patch_height and stride_width < patch_width)

# PATH_TEMP = "temp/"
# temp_file = "data.hdf5"
# dataset_path = PATH_TEMP

import h5py
import numpy as np
from PIL import Image

from os import listdir
from os.path import isfile, join

#Python
import numpy as np
# import ConfigParser

#Keras
from keras.models import model_from_json
from keras.models import Model

import sys
sys.path.insert(0, './lib/')
# help_functions.py
# from help_functions import load_hdf5, write_hdf5, rgb2gray, group_images, visualize, masks_Unet, pred_to_imgs
from help_functions import rgb2gray, group_images, visualize, masks_Unet, pred_to_imgs

# extract_patches.py
# from extract_patches import recompone
from extract_patches import recompone_overlap
from extract_patches import paint_border
from extract_patches import paint_border_overlap
from extract_patches import extract_ordered_overlap
from extract_patches import kill_border
from extract_patches import pred_not_only_FOV
from extract_patches import get_data_testing
from extract_patches import get_data_testing_overlap

# pre_processing.py
from pre_processing import my_PreProc

# def write_hdf5(arr,outfile):
#     with h5py.File(outfile,"w") as f:
#         f.create_dataset("image", data=arr, dtype=arr.dtype)

def get_list_files(imgs_dir):
    list_files = [f for f in listdir(imgs_dir) if isfile(join(imgs_dir, f))]
    list_files.remove(".gitignore")
    return list_files

# def get_datasets(imgs_dir):
    
#     Nimgs = len([f for f in listdir(imgs_dir) if isfile(join(imgs_dir, f))])
#     imgs = np.empty((Nimgs,height,width,channels))
#     for path, subdirs, files in os.walk(imgs_dir): #list all files, directories in the path
#         for i in range(len(files)):
#             print ( ("image: " +files[i]))
            
#             ### write some lines of code to:
#             ### - check if image's size is different to requirement, then resize and save to output folder
#             ### - else: make a copy to output folder
            
#             img = Image.open(imgs_dir+files[i])
#             imgs[i] = np.asarray(img)    
#     imgs = np.transpose(imgs,(0,3,1,2))
#     assert(imgs.shape == (Nimgs,channels,height,width))
#     return imgs, files

def get_image(path_image):
    ### load image
    img = Image.open(path_image)
    img_array = np.asarray(img)
    # print("STD load: " + str(np.std(img_array)))
    height, width, channels = img_array.shape

    ### add image to 4d matrix
    Nimgs = 1 # this matrix contains only 1 image
    img_4d = np.empty((Nimgs,height,width,channels))
    img_4d[0] = img_array
    img_4d = np.transpose(img_4d,(0,3,1,2))
    assert(img_4d.shape == (Nimgs,channels,height,width))
    
    # print("STD img_4d: " + str(np.std(img_4d)))
    return img_4d

def get_data(imgs_test, patch_height, patch_width, stride_height, stride_width):
    test_imgs_original = imgs_test
    test_imgs = my_PreProc(test_imgs_original)
    # test_imgs = test_imgs[:,:,:,:]
    test_imgs = paint_border_overlap(test_imgs, patch_height, patch_width, stride_height, stride_width)

#     print ( "\ntest images shape:")
#     print ( test_imgs.shape)
#     print ( "range (min-max): " +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))
   
    #extract the TEST patches from the full images
    patches_imgs_test = extract_ordered_overlap(test_imgs,patch_height,patch_width,stride_height,stride_width)

#     print ( "\ntest PATCHES images shape:")
#     print ( patches_imgs_test.shape)
#     print ( "range (min-max): " +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test)))
    
    return patches_imgs_test, test_imgs.shape[2], test_imgs.shape[3]
  
def createDir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)
    else:
        print ( directory + " already exists")

def predict_and_save(list_input_images, model):    
    print("-------------------------------------")
    print("Found " + str(len(list_input_images)) + " images in directory " + PATH_INPUT + "\n")
    print("It should take minutes per image, depending on your CPU capacity. You can go out and come back later ;)\n")

    for input_image in list_input_images:
        start_time_image = time.time()

        print("Working on " + input_image)

        img_test = get_image(PATH_INPUT + input_image)

        ### get image sizes
        # test_imgs_orig = img_test
        full_img_height = img_test.shape[2]
        full_img_width = img_test.shape[3]
        print("Size: " + str(full_img_height) + "x" + str(full_img_width))

        ### Images to patches:
        patches_imgs_test = None
        new_height = None
        new_width = None
        
        patches_imgs_test, new_height, new_width = get_data(
            imgs_test = img_test,
            patch_height = patch_height,
            patch_width = patch_width,
            stride_height = stride_height,
            stride_width = stride_width
        )

        ### Calculate the predictions
        print ("Computing output...")
        predictions = model.predict(patches_imgs_test, batch_size=32, verbose=2)
        # print ( "predicted images size :")
        # print ( predictions.shape)

        ### Patches back to image
        pred_patches = pred_to_imgs(predictions, patch_height, patch_width, "original")

        pred_imgs = None
        pred_imgs = recompone_overlap(pred_patches, new_height, new_width, stride_height, stride_width)        

        pred_imgs = pred_imgs[:,:,0:full_img_height,0:full_img_width]

        assert(pred_imgs.shape[0] == 1)

        # N_predicted = pred_imgs.shape[0]
        # group = 1

        # Save predictions to files
        # for i in range(int(N_predicted)):
        pred_stripe = group_images(pred_imgs[:,:,:,:],1)
        # file_name =  input_image
        visualize(pred_stripe, "output/" + input_image[0:len(input_image)-4] + "_pred",mode="jpg")

        elapsed_time_image = time.time() - start_time_image
        print ("Time consumed: " + str(int(elapsed_time_image)) + "s\n")


list_input_images = get_list_files(PATH_INPUT)

### Load model
model = model_from_json(open(architechture_path).read())
model.load_weights(model_path)

predict_and_save(list_input_images, model)

print ( "All done! Please check the output folder")